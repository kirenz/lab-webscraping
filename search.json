[
  {
    "objectID": "requirements.html",
    "href": "requirements.html",
    "title": "Requirements",
    "section": "",
    "text": "To start this lab, you‚Äôll need the following environments:"
  },
  {
    "objectID": "requirements.html#webscraping-tools",
    "href": "requirements.html#webscraping-tools",
    "title": "Requirements",
    "section": "Webscraping tools",
    "text": "Webscraping tools\nüíæ Download the Google Chrome Web-Browser\nüíæ Install the SelectorGadget extension in Google Chrome"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Download the Jupyter Notebooks in your local folder webscraping.\n\nTask 1:"
  },
  {
    "objectID": "slides/slides.html#text",
    "href": "slides/slides.html#text",
    "title": "foo",
    "section": "Text",
    "text": "Text\n\na ü§ñ\n\nabc\n\n\n\n\nb\nc1\n\nüìö Required reading: A & B (2023)\nhttps://arxiv.org/pdf/2303.12712.pdf\n\nRussell & Norvig, 2009"
  },
  {
    "objectID": "slides/slides.html#image",
    "href": "slides/slides.html#image",
    "title": "foo",
    "section": "Image",
    "text": "Image"
  },
  {
    "objectID": "slides/slides.html#video",
    "href": "slides/slides.html#video",
    "title": "foo",
    "section": "Video",
    "text": "Video"
  },
  {
    "objectID": "slides/slides.html#a-lot-of-text",
    "href": "slides/slides.html#a-lot-of-text",
    "title": "foo",
    "section": "A lot of text",
    "text": "A lot of text\nSmaller heading"
  },
  {
    "objectID": "slides/slides.html#background-image",
    "href": "slides/slides.html#background-image",
    "title": "foo",
    "section": "Background image",
    "text": "Background image\nabc"
  },
  {
    "objectID": "slides/slides.html#code",
    "href": "slides/slides.html#code",
    "title": "foo",
    "section": "Code",
    "text": "Code\n1print('Hello World')\n2for i in LIST:\n  df[i] = df[i].astype('cat')\n\n1\n\nPrint Hello World, and then,\n\n2\n\ntransform all columns in the LIST element to categorical variables"
  },
  {
    "objectID": "code/4_analyse.html",
    "href": "code/4_analyse.html",
    "title": "Sentiment-Analyse",
    "section": "",
    "text": "# Import der erforderlichen Bibliotheken gem√§√ü PEP8\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n\n[nltk_data] Downloading package punkt to /Users/jankirenz/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/jankirenz/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyse abgeschlossen und Ergebnisse gespeichert.\n\n\n\n\n\n\n\n\n\nBegriff\nH√§ufigkeit\nStudiengang\n\n\n\n\n0\nradio\n24\nMittweida\n\n\n1\npraktisch\n15\nMittweida\n\n\n2\nstadt\n12\nMittweida\n\n\n3\njournalismus\n12\nMittweida\n\n\n4\nverf√ºgung\n11\nMittweida\n\n\n5\nmedienforum\n11\nMittweida\n\n\n6\ntv\n11\nMittweida\n\n\n7\npraktischen\n11\nMittweida\n\n\n8\ncampusfestival\n10\nMittweida\n\n\n9\nfernsehstudio\n10\nMittweida\n\n\n10\nit\n20\nOMM\n\n\n11\nprogrammieren\n13\nOMM\n\n\n12\nspezialisieren\n12\nOMM\n\n\n13\ninformatik\n10\nOMM\n\n\n14\nperfekt\n9\nOMM\n\n\n15\nwahl\n8\nOMM\n\n\n16\nmischung\n7\nOMM\n\n\n17\nmedienwelt\n7\nOMM\n\n\n18\nbreit\n7\nOMM\n\n\n19\nbem√ºht\n7\nOMM\n\n\n20\nunterricht\n28\nMacromedia\n\n\n21\nunternehmen\n20\nMacromedia\n\n\n22\nhilfsbereit\n18\nMacromedia\n\n\n23\nf√§cher\n18\nMacromedia\n\n\n24\ngef√ºhl\n17\nMacromedia\n\n\n25\ncorona\n17\nMacromedia\n\n\n26\nbereits\n17\nMacromedia\n\n\n27\nfreundlich\n16\nMacromedia\n\n\n28\nw√ºnschen\n16\nMacromedia\n\n\n29\nwurden\n16\nMacromedia\n\n\n30\nschwerpunkt\n19\nDMW\n\n\n31\nbreit\n17\nDMW\n\n\n32\nwirtschaft\n16\nDMW\n\n\n33\nprofs\n16\nDMW\n\n\n34\nauswahl\n14\nDMW\n\n\n35\nmodern\n13\nDMW\n\n\n36\npraxisbezug\n12\nDMW\n\n\n37\npraxissemester\n12\nDMW\n\n\n38\nkonzeption\n12\nDMW\n\n\n39\nmw\n12\nDMW\n\n\n\n\n\n\n\n\n\n# Herunterladen der NLTK-Ressourcen\nnltk.download('punkt')\nnltk.download('stopwords')\n\n\n\n# Zus√§tzliche Stopw√∂rter definieren\nadditional_stopwords = set([\n    'studium', 'studiengang', 'hochschule', 'uni', 'universit√§t', 'semester', \n    'student', 'studieren', 'studierende', 'fach', 'lehrveranstaltung', 'professor', \n    'dozent', 'viele', 'dabei', 'ab', 'jedoch', 'auch', 'immer', 'w√§hrend', 'mehr', \n    'bisher', 'obwohl', 'zudem', 'hochschule', 'studium', 'studiengang', 'semester', 'erfahrungsbericht', \n    'weiterlesen', 'dozenten', 'gut', 'viele', 'immer', 'gibt', 'hdm', \n    'macromedia', 'mittweida', 'vorlesungen',  'aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', \n    'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', \n    'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', \n    'die', 'das', 'dass', 'da√ü', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', \n    'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', \n    'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', \n    'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', \n    'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', \n    'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'f√ºr', 'gegen', 'gewesen', \n    'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', \n    'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', \n    'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', \n    'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'k√∂nnen', 'k√∂nnte', \n    'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', \n    'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', \n    'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', \n    'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', \n    'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', '√ºber', 'um', 'und', \n    'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', \n    'vor', 'w√§hrend', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', \n    'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', \n    'wird', 'wirst', 'wo', 'wollen', 'wollte', 'w√ºrde', 'w√ºrden', 'zu', 'zum', 'zur', 'zwar', \n    'zwischen'\n])\n\n# Hinzuf√ºgen der neuen Stopw√∂rter\nadditional_stopwords.update([\n    'wirklich', 'lernt', 'spa√ü', 'vielen', 'macht', 'professoren', 'erfahrungen', \n    'schon', 'lehrveranstaltungen', 'interessant', 'wissen', 'studiums', 'verschiedene', \n    'allerdings', 'zufrieden', 'bereich', 'einfach', 'm√∂glichkeit', 'lernen', 'top', \n    'theorie', 'bekommt', 'arbeiten', 'teilweise', 'kompetent', 'mal', 'themen', 'zeit', \n    'studieninhalte', 'gemacht', 'module', 'oft', 'tolle', 'besonders', 'denen', 'wurde', \n    'erfahrung', 'daf√ºr', 'management', 'empfehlen', 'konnte', 'meisten', 'ausgestattet', \n    'au√üerdem', 'm√∂chte', 'grundlagen', 'eher', 'gerade', 'fragen', 'dadurch', \n    'super', 'medien', 'projekte', 'studenten', 'gute', 'inhalte', 'organisation', \n    'leider', 'bietet', 'praxis', 'm√∂glichkeiten', 'ausstattung', 'ersten', 'bwl', \n    'projekten', 'medienmanagement', 'campus', 'online', 'auslandssemester', \n    'grundstudium', 'medienwirtschaft', 'marketing', 'kurse', 'schnell', 'gestaltet', \n    'kommt', 'technik', 'veranstaltungen', 'sagen', 'bereichen', 'praxisnah', \n    'wenig', 'besser', 'neben', 'semestern', 'sp√§ter', 'ganz', 'studierenden', \n    'berufsleben', 'vermittelt', 'nat√ºrlich', 'direkt', 'erst', 'finde', 'geht', \n    'praktische', 'kommen', 'medienbranche', 'richtung', 'gehen', 'bereiche', 'wer', 'stets',\n    'fast', 'bereit', 'sammeln', 'gro√üe', 'l√§sst', 'richtig', 'richtige', 'genau', 'neue', 'bringen',\n    'leben', 'w√§hlen', 'etc', 'erh√§lt', 'gab', 'm√∂glich', 'bekommen', 'hauptstudium',\n    'eigenen', 'einblick', 'f√ºhlt', 'vertiefung', 'omm', 'mhmk', 'echt'\n])\n\n\n\n# Textbereinigungsfunktion\ndef clean_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [word for word in tokens if word not in stopwords.words('german')]\n    tokens = [word for word in tokens if word not in additional_stopwords]\n    tokens = [word for word in tokens if word.isalnum()]\n    return ' '.join(tokens)\n\n\n\n# Funktion zur Generierung von Wortwolken\ndef generate_wordcloud(text, title):\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n\n\n\n# Lade die CSV-Dateien\nmittweida_df = pd.read_csv('medienmanagement_mittweida_reviews.csv')\nhdm_omm_df = pd.read_csv('online_medien_management_hdm_reviews.csv')\nmacromedia_df = pd.read_csv('medien_und_kommunikationsmanagement_macromedia_hochschule_reviews.csv')\nhdm_mw_df = pd.read_csv('digital_und_medienwirtschaft_hdm_reviews.csv')\n\n# F√ºge eine Spalte f√ºr den Studiengang hinzu\nmittweida_df['Studiengang'] = 'Mittweida'\nhdm_omm_df['Studiengang'] = 'OMM'\nmacromedia_df['Studiengang'] = 'Macromedia'\nhdm_mw_df['Studiengang'] = 'DMW'\n\n# Kombiniere die DataFrames f√ºr die Analyse\nall_reviews_df = pd.concat([mittweida_df, hdm_omm_df, macromedia_df, hdm_mw_df], ignore_index=True)\n\n# Bereinige die Texte in den Reviews\nall_reviews_df['cleaned_review'] = all_reviews_df['Review'].apply(clean_text)\n\n# Generiere eine Wortwolke pro Studiengang\nfor studiengang in all_reviews_df['Studiengang'].unique():\n    studiengang_reviews = ' '.join(all_reviews_df[all_reviews_df['Studiengang'] == studiengang]['cleaned_review'])\n    generate_wordcloud(studiengang_reviews, f'{studiengang} - H√§ufigste Begriffe nach Bereinigung')\n\n# Speichere die bereinigten Daten in einer CSV-Datei\nall_reviews_df.to_csv('all_reviews_cleaned_v2.csv', index=False)\nprint(\"Analyse abgeschlossen und Ergebnisse gespeichert.\")\n\n# Funktion zur Extraktion der h√§ufigsten Begriffe\ndef get_top_terms(text, top_n=10):\n    tokens = word_tokenize(text)\n    counter = Counter(tokens)\n    most_common = counter.most_common(top_n)\n    return pd.DataFrame(most_common, columns=['Begriff', 'H√§ufigkeit'])\n\n# Erstelle eine DataFrame f√ºr die wichtigsten Begriffe pro Studiengang\ntop_terms_per_studiengang = pd.DataFrame()\n\nfor studiengang in all_reviews_df['Studiengang'].unique():\n    studiengang_reviews = ' '.join(all_reviews_df[all_reviews_df['Studiengang'] == studiengang]['cleaned_review'])\n    top_terms = get_top_terms(studiengang_reviews, top_n=10)\n    top_terms['Studiengang'] = studiengang\n    top_terms_per_studiengang = pd.concat([top_terms_per_studiengang, top_terms], ignore_index=True)\n\n# Zeige die Tabelle als pandas DataFrame an\ntop_terms_per_studiengang"
  },
  {
    "objectID": "code/1_scraping_quotes.html",
    "href": "code/1_scraping_quotes.html",
    "title": "Web Scraping in Python with Beautiful Soup, Requests and pandas",
    "section": "",
    "text": "Wichtige Hinweise\nNAME = \"\"\nimport IPython\nassert IPython.version_info[0] &gt;= 3, \"Your version of IPython is too old, please update it.\""
  },
  {
    "objectID": "code/1_scraping_quotes.html#prerequisites",
    "href": "code/1_scraping_quotes.html#prerequisites",
    "title": "Web Scraping in Python with Beautiful Soup, Requests and pandas",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo start this tutorial, you need:\n\nSome basic understanding of HTML and CSS and CSS selectors.\nGoogle‚Äôs web browser Chrome and the Chrome extension SelectorGadget\nTo know how to use Chrome DevTools\n\n\nTo learn more about HTML, CSS, Chrome DevTools and the Selector Gadget, follow the instructions in this web scraping basics tutorial."
  },
  {
    "objectID": "code/1_scraping_quotes.html#setup",
    "href": "code/1_scraping_quotes.html#setup",
    "title": "Web Scraping in Python with Beautiful Soup, Requests and pandas",
    "section": "Setup",
    "text": "Setup\n\nimport pandas as pd\n\nimport requests\nfrom bs4 import BeautifulSoup"
  },
  {
    "objectID": "code/1_scraping_quotes.html#scrape-website-with-requests",
    "href": "code/1_scraping_quotes.html#scrape-website-with-requests",
    "title": "Web Scraping in Python with Beautiful Soup, Requests and pandas",
    "section": "Scrape website with Requests",
    "text": "Scrape website with Requests\n\nFirst, we use requests to scrape the website (using a GET request).\nrequests.get() fetches all the content from a particular website and returns a response object (we call it html):\n\n\nurl = 'http://quotes.toscrape.com/'\n\nHint:\n___ = .get()\n\n# YOUR CODE HERE\nraise NotImplementedError()\n\n\nCheck if the response was succesful (with .status_code):\n\n\nhtml.status_code\n\n\nResponse 200 means that the request has succeeded.\n\n\n\"\"\"Run this cell to check that yor code returns the correct output\"\"\"\nassert html.status_code == 200\nassert html.url == \"http://quotes.toscrape.com/\""
  },
  {
    "objectID": "code/1_scraping_quotes.html#investigate-html-with-beautiful-soup",
    "href": "code/1_scraping_quotes.html#investigate-html-with-beautiful-soup",
    "title": "Web Scraping in Python with Beautiful Soup, Requests and pandas",
    "section": "Investigate HTML with Beautiful Soup",
    "text": "Investigate HTML with Beautiful Soup\n\nWe can use the response object to access certain features such as content, text, headers, etc.\nIn our example, we only want to obtain text from the object.\nTherefore, we use html.text which only returns the text of the response.\nRunning html.text through BeautifulSoup using the html.parser gives us a Beautiful Soup object:\n\n\nsoup = BeautifulSoup(html.text, 'html.parser')\n\n\nsoup represents the document as a nested data structure:\n\n\nprint(soup.prettify())\n\nNext, we take a look at some ways to navigate that data structure.\n\nGet all text\n\nA common task is extracting all the text from a page (since the output is quite large, we don‚Äôt actually print the output of the following function):\n\n\n# print(soup.get_text())\n\n\n\nInvestigate title\n\nPrint the complete HTML title (.title):\n\n\nsoup.title\n\n\nShow name of the title tag (.title.name):\n\n\n# YOUR CODE HERE\nraise NotImplementedError()\n\n\n\"\"\"Run this cell to check that yor code returns the correct output\"\"\"\nassert soup.title.name == \"title\"\n\n\nOnly print the text of the title (title.string):\n\n\n# YOUR CODE HERE\nraise NotImplementedError()\n\n\n\"\"\"Run this cell to check that yor code returns the correct output\"\"\"\nassert soup.title.string == \"Quotes to Scrape\"\n\n\nShow the name of the parent tag of title:\n\n\nsoup.title.parent.name\n\n\n\nInvestigate hyperlinks\n\nShow the first hyperlink in the document:\n\n\nsoup.a\n\n\n\nInvestigate a text element\n\nsoup.span.text\n\n\n\nExtract specific elements with find and find_all\n\nSince there are many div tags in HTML, we can‚Äôt use the previous approaches to extract relevant information.\nInstead, we need to use the find and find_all methods which you can use to extract specific HTML tags from the web page.\nThis methods can be used to retrieve all the elements on the page that match our specifications.\nLet‚Äôs say our goal is to obtain all quotes, authors and tags from the website ‚ÄúQuotes to Scrape‚Äù.\nWe want to store all information in a pandas dataframe (every row should contain a quote as well as the corresponding author and tags).\nFirst, we use SelectorGadget in Google Chrome to inspect the website.\n\n\nReview the web scraping basics tutorial to learn how inspect websites.\n\n\nExtract all quotes\nTask: Extract all quotes\n\nFirst, we use the div class ‚Äúquote‚Äù to retrieve all relevant information regarding the quotes:\n\n\nquotes_all = soup.find_all('div', {'class': 'quote'})\n\n\nquotes_all\n\n\nNext, we can iterate through our new quotes_all object and extract only the text of the quotes:\n\nwe want to store all text quotes in a new array called quotes_text (you need top provide an empty list)\nTo extract the quotes, note that the text of the quotes are available in the tag span as ‚Äúclass:text‚Äù (see output above))\nfinally, we can use the method .text to make sure we only extract text\n\n\nSome hints:\n# create empty array\nquotes_text = []\n\n# use for loop to write quotes in quotes_text with append\nfor i in ___:\n   ___.append((___.find('___', {'___':'___'})).___)\n\n# YOUR CODE HERE\nraise NotImplementedError()\n\n\n\"\"\"Run this cell to check that yor code returns the correct output\"\"\"\nassert quotes_text[0] == \"‚ÄúThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.‚Äù\"\n\nTake a look at quotes_text\n\n # quotes_text\n\n\nNext, we want to store the data in a pandas dataframe (to make later data preprocessing steps easier)\n\n\ndf_quotes = pd.DataFrame({\"quote\" : quotes_text})\ndf_quotes\n\n\n\nExtract all authors\nTask: Extract all authors\n\nsoup\n\n\nIn this example, we don‚Äôt want to create a new object (like quotes_all) as an intermediate step.\nInstead, we use a different approach:\n\ncreate an emtpty array mit the name authors_text\nuse the soup object and implement the find_all() function in a for loop to extract the authors (take a look at the code where we created quotes_all):\n\n\nHint:\n___ = []\n\nfor i in ___.___(\"___\",{\"___\": \"___\"}):\n    ___.___((___.___(\"___\", {\"___\": \"___\"})).___)\n\n# YOUR CODE HERE\nraise NotImplementedError()\n\n\n\"\"\"Run this cell to check that yor code returns the correct output\"\"\"\nassert authors_text[0] == \"Albert Einstein\"\n\nWe create a new dataframe:\n\ncall the dataframe: df_authors\nname the column: author\n\n\n# YOUR CODE HERE\nraise NotImplementedError()\n\n\n\"\"\"Run this cell to check that yor code returns the correct output\"\"\"\nassert df_authors.iloc[0,0] == \"Albert Einstein\"\n\nWe can use a left join to combine the two dataframes:\n\ndf1 = df_quotes.join(df_authors)\ndf1\n\n\n\nExtract all tags\nTask: Extract all tags\n\nWe use the same process as in the extraction of the authors to obtain the tags\nInformation about the tags is available in the class ‚Äútags‚Äù.\nThis time, we need to extract the ‚Äúcontent‚Äù from ‚Äúmeta‚Äù and return it as array (since there are multiple entries per quote)\nCall the array tags_text\n\nHint:\n\n___ = []\n\nfor i in ___.___(\"___\",{\"___\": \"___\"}):\n    ___.___((___.___(\"___\"))['___'])\n    \n\n# YOUR CODE HERE\nraise NotImplementedError()\n\n\n\"\"\"Run this cell to check that yor code returns the correct output\"\"\"\nassert tags_text[0] == \"change,deep-thoughts,thinking,world\"\n\nWe create a new dataframe:\n\ncall the dataframe: df_tags\nname the column: tags\n\n\ndf_tags = pd.DataFrame({\"tags\" : tags_text})"
  },
  {
    "objectID": "code/1_scraping_quotes.html#create-dataframe-for-all-quotes-authors-and-tags",
    "href": "code/1_scraping_quotes.html#create-dataframe-for-all-quotes-authors-and-tags",
    "title": "Web Scraping in Python with Beautiful Soup, Requests and pandas",
    "section": "Create dataframe for all quotes, authors and tags",
    "text": "Create dataframe for all quotes, authors and tags\nFinally, we can combine all information in one single dataframe\n\ndf2 = df1.join(df_tags)\ndf2\n\n\nNext, we want to store ALL quotes with the corresponding authors and tags information in a pandas dataframe.\nNote that the site has a total of ten pages and we want to collect the data from all of them (we only extracted content from the first page so far).\nThe website‚Äôs URL address is structured as follows:\n\npage 1: https://quotes.toscrape.com/page/1/\npage 2: https://quotes.toscrape.com/page/2/\n‚Ä¶\npage 10: https://quotes.toscrape.com/page/10/\n\nThis means we can use the part ‚Äúhttps://quotes.toscrape.com/page/‚Äù as root and iterate over the pages 1 to 10.\n\nWe will proceed as follows:\n\nStore the root url without the page number as a variable called root.\nPrepare three empty arrays: quotes, authors and tags.\nCreate a loop that ranges from 1‚Äì10 to iterate through every page on the site. Take a close look at this definition of the range() function\nAppend the scraped data to our arrays.\nCreate a dataframe\n\n\nNote that we use almost the same code as before\n\nHint:\n# store root url without page number... needs to end with /\nroot = 'http://___/'\n\n# create empty arrays\n\n\n\n# loop over page 1 to 10 with range() - noteice the information above\nfor pages in range(__,__): \n        \n        html = requests.get(___ + str(pages))\n        \n        soup = BeautifulSoup(___.text)    \n\n        for i in soup.findAll(\"div\",{\"class\":\"quote\"}):\n                 quotes.append((i.find(\"span\",{\"class\":\"text\"})).text)  \n   \n        for j in soup.findAll(\"div\",{\"class\":\"quote\"}):\n                 authors.append((j.find(\"small\",{\"class\":\"author\"})).text)    \n        \n        for k in soup.findAll(\"div\",{\"class\":\"tags\"}):\n                 tags.append((k.find(\"meta\"))['content'])\n\n# create dataframe\ndf = pd.DataFrame(\n    {'Quotes':quotes,\n     'Authors':authors,\n     'Tags':tags\n    })\n\n# YOUR CODE HERE\nraise NotImplementedError()\n\n\n\"\"\"Run this cell to check that yor code returns the correct output\"\"\"\nassert df.iloc[0, 2] == \"change,deep-thoughts,thinking,world\"\nassert len(df) == 100\nassert df.iloc[0, 1] == \"Albert Einstein\"\n\n\nShow result\n\n\ndf.head()\n\n\nCongratulations! You have successfully completed this tutorial."
  },
  {
    "objectID": "code/3_analyse.html",
    "href": "code/3_analyse.html",
    "title": "Sentiment-Analyse",
    "section": "",
    "text": "Create virtual environment (once)\nActivate environment\nInstall modules (once)\nImport modules (always)\n# Import der erforderlichen Bibliotheken gem√§√ü PEP8\nimport pandas as pd\nimport nltk\nimport spacy\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom spacytextblob.spacytextblob import SpacyTextBlob\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport altair as alt\n# Herunterladen der NLTK-Ressourcen\nnltk.download('punkt')\nnltk.download('stopwords')"
  },
  {
    "objectID": "code/3_analyse.html#daten-laden",
    "href": "code/3_analyse.html#daten-laden",
    "title": "Sentiment-Analyse",
    "section": "Daten laden",
    "text": "Daten laden\n\n\n# Lade die CSV-Dateien\nmittweida_df = pd.read_csv('medienmanagement_mittweida_reviews.csv')\nhdm_omm_df = pd.read_csv('online_medien_management_hdm_reviews.csv')\nmacromedia_df = pd.read_csv('medien_und_kommunikationsmanagement_macromedia_hochschule_reviews.csv')\nhdm_mw_df = pd.read_csv('digital_und_medienwirtschaft_hdm_reviews.csv')\n\n\n\n# F√ºge eine Spalte f√ºr den Studiengang hinzu\nmittweida_df['Studiengang'] = 'Mittweida'\nhdm_omm_df['Studiengang'] = 'OMM'\nmacromedia_df['Studiengang'] = 'Macromedia'\nhdm_mw_df['Studiengang'] = 'DMW'\n\n\n\n# Kombiniere die DataFrames f√ºr die Analyse\nall_reviews_df = pd.concat([mittweida_df, hdm_omm_df, macromedia_df, hdm_mw_df], ignore_index=True)"
  },
  {
    "objectID": "code/3_analyse.html#stoppw√∂rter-definieren",
    "href": "code/3_analyse.html#stoppw√∂rter-definieren",
    "title": "Sentiment-Analyse",
    "section": "Stoppw√∂rter definieren",
    "text": "Stoppw√∂rter definieren\n\n\n# Zus√§tzliche Stopw√∂rter definieren\nadditional_stopwords = set([\n    'studium', 'studiengang', 'hochschule', 'uni', 'universit√§t', 'semester', \n    'student', 'studieren', 'studierende', 'fach', 'lehrveranstaltung', 'professor', \n    'dozent', 'viele', 'dabei', 'ab', 'jedoch', 'auch', 'immer', 'w√§hrend', 'mehr', \n    'bisher', 'obwohl', 'zudem', 'hochschule', 'studium', 'studiengang', 'semester', 'erfahrungsbericht', \n    'weiterlesen', 'dozenten', 'gut', 'viele', 'immer', 'gibt', 'hdm', \n    'macromedia', 'mittweida', 'vorlesungen',  'aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', \n    'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', \n    'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', \n    'die', 'das', 'dass', 'da√ü', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', \n    'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', \n    'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', \n    'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', \n    'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', \n    'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'f√ºr', 'gegen', 'gewesen', \n    'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', \n    'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', \n    'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', \n    'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'k√∂nnen', 'k√∂nnte', \n    'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', \n    'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', \n    'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', \n    'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', \n    'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', '√ºber', 'um', 'und', \n    'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', \n    'vor', 'w√§hrend', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', \n    'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', \n    'wird', 'wirst', 'wo', 'wollen', 'wollte', 'w√ºrde', 'w√ºrden', 'zu', 'zum', 'zur', 'zwar', \n    'zwischen'\n])"
  },
  {
    "objectID": "code/3_analyse.html#textbereiningung",
    "href": "code/3_analyse.html#textbereiningung",
    "title": "Sentiment-Analyse",
    "section": "Textbereiningung",
    "text": "Textbereiningung\n\n\n# Textbereinigungsfunktion\ndef clean_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [word for word in tokens if word not in stopwords.words('german')]\n    tokens = [word for word in tokens if word not in additional_stopwords]\n    tokens = [word for word in tokens if word.isalnum()]\n    return ' '.join(tokens)\n\n\n\n# Bereinige die Texte in den Reviews\nall_reviews_df['cleaned_review'] = all_reviews_df['Review'].apply(clean_text)\n\n\n\n# Funktion zur Generierung von Wortwolken\ndef generate_wordcloud(text, title):\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n\n\n\n# Lade das deutsche Sprachmodell und f√ºge die SpacyTextBlob-Komponente hinzu\nnlp = spacy.load('de_core_news_sm')\nspacy_text_blob = SpacyTextBlob(nlp)\nnlp.add_pipe('spacytextblob')\n\n\n\n# Sentiment-Analyse-Funktion\ndef get_sentiment(text):\n    doc = nlp(text)\n    return doc._.polarity\n\n\n\n# F√ºhre die Sentiment-Analyse durch\nall_reviews_df['sentiment'] = all_reviews_df['cleaned_review'].apply(get_sentiment)\n\n\n\n# √úberpr√ºfung der Sentiment-Analyse durch manuelle √úberpr√ºfung einiger Beispiele\nprint(\"Manuelle √úberpr√ºfung einiger Bewertungen:\")\nprint(all_reviews_df[(all_reviews_df['sentiment'] &lt; 0)].head(10)['Review'])\n\n\n\n# Positive und negative Bewertungen trennen und Wortwolken erstellen\nfor studiengang in all_reviews_df['Studiengang'].unique():\n    pos_reviews = ' '.join(all_reviews_df[(all_reviews_df['Studiengang'] == studiengang) & (all_reviews_df['sentiment'] &gt; 0)]['cleaned_review'])\n    neg_reviews = ' '.join(all_reviews_df[(all_reviews_df['Studiengang'] == studiengang) & (all_reviews_df['sentiment'] &lt; 0)]['cleaned_review'])\n    \n    # Wortwolke f√ºr positive Bewertungen\n    generate_wordcloud(pos_reviews, f'{studiengang} - Positive Reviews')\n    \n    # Wortwolke f√ºr negative Bewertungen\n    generate_wordcloud(neg_reviews, f'{studiengang} - Negative Reviews')\n\n\n\n# Kategorisierung der Bewertungen\ncategories = ['Dozenten', 'Technik', 'Ausstattung', 'Organisation', 'Praxisbezug']\ncategory_keywords = {\n    'Dozenten': ['dozent', 'professor', 'lehrkraft'],\n    'Technik': ['technik', 'computer', 'software', 'hardware'],\n    'Ausstattung': ['ausstattung', 'raum', 'labor', 'bibliothek'],\n    'Organisation': ['organisation', 'verwaltung', 'koordinierung'],\n    'Praxisbezug': ['praxis', 'projekt', 'praktikum', 'anwendung']\n}\n\n\n# Funktion zur Kategorisierung der Bewertungen\ndef categorize_review(review):\n    for category, keywords in category_keywords.items():\n        if any(keyword in review.lower() for keyword in keywords):\n            return category\n    return 'Sonstiges'\n\n\n\nall_reviews_df['category'] = all_reviews_df['cleaned_review'].apply(categorize_review)\n\n\n\n# Durchschnittliches Sentiment pro Kategorie und Studiengang\ncategory_sentiment = all_reviews_df.groupby(['Studiengang', 'category'])['sentiment'].mean().unstack()\n\n\n\n# Visualisierung der durchschnittlichen Sentiments pro Kategorie und Studiengang mit Altair\ncategory_sentiment_reset = category_sentiment.reset_index().melt(id_vars='Studiengang', var_name='Kategorie', value_name='Sentiment')\n\n\n\nbar_chart = alt.Chart(category_sentiment_reset).mark_bar().encode(\n    x=alt.X('Kategorie:N', title='Kategorie'),\n    y=alt.Y('Sentiment:Q', title='Durchschnittliches Sentiment'),\n    color='Studiengang:N',\n    column='Studiengang:N'\n).properties(\n    title='Durchschnittliches Sentiment pro Kategorie und Studiengang'\n).configure_title(\n    fontSize=20,\n    anchor='start',\n    color='gray'\n).configure_axis(\n    labelFontSize=12,\n    titleFontSize=14\n)\n\nbar_chart.display()\n\n\n\n# Relative Gegen√ºberstellung der Bewertungen\nrelative_strengths = all_reviews_df.groupby(['Studiengang', 'category'])['sentiment'].mean().unstack().rank(axis=0)\n\n\n\n# Visualisierung der relativen St√§rken und Schw√§chen mit Altair\nrelative_strengths_reset = relative_strengths.reset_index().melt(id_vars='Studiengang', var_name='Kategorie', value_name='Rang')\n\n\n\nheatmap = alt.Chart(relative_strengths_reset).mark_rect().encode(\n    x=alt.X('Kategorie:N', title='Kategorie'),\n    y=alt.Y('Studiengang:N', title='Studiengang'),\n    color=alt.Color('Rang:Q', scale=alt.Scale(domain=[1, 4], range=['red', 'green']), title='Rang'),\n    tooltip=['Studiengang', 'Kategorie', 'Rang']\n).properties(\n    title='Relative St√§rken und Schw√§chen pro Kategorie und Studiengang'\n).configure_title(\n    fontSize=20,\n    anchor='start',\n    color='gray'\n).configure_axis(\n    labelFontSize=12,\n    titleFontSize=14\n).encode(\n    text=alt.Text('Rang:Q')\n)\n\nheatmap.display()\n\n\n\n# Speichere die Ergebnisse in einer CSV-Datei\nall_reviews_df.to_csv('all_reviews_with_sentiment_and_categories.csv', index=False)\nprint(\"Analyse abgeschlossen und Ergebnisse gespeichert.\")\n\n# Interpretationen der Ergebnisse\ninterpretations = \"\"\"\n### Interpretation der Ergebnisse:\n\n#### Sentiment-Analyse:\nDie Sentiment-Analyse ergab gemischte Ergebnisse mit positiven und negativen Bewertungen. Die manuelle √úberpr√ºfung einiger als negativ eingestufter Bewertungen zeigt, dass nicht alle Bewertungen, die als negativ eingestuft wurden, tats√§chlich negativen Inhalt haben. Dies k√∂nnte auf die T√ºcken der Sentiment-Analyse hinweisen, insbesondere bei der Verarbeitung von Ironie oder komplexen S√§tzen.\n\n#### Durchschnittliches Sentiment pro Kategorie und Studiengang:\nDie Visualisierung des durchschnittlichen Sentiments pro Kategorie und Studiengang zeigt deutliche Unterschiede zwischen den Studieng√§ngen. Beispielsweise:\n- **OMM**: Zeigt insgesamt positive Bewertungen, insbesondere in den Kategorien Dozenten und Praxisbezug.\n- **Mittweida**: Hat relativ ausgewogene Bewertungen, aber Schw√§chen in der Kategorie Organisation.\n\n#### Relative St√§rken und Schw√§chen:\nDie Heatmap zur relativen Analyse zeigt, wie die einzelnen Studieng√§nge im Vergleich zu den anderen in den verschiedenen Kategorien abschneiden. Ein hoher Rang deutet auf relative St√§rken hin, w√§hrend ein niedriger Rang auf relative Schw√§chen hinweist.\n- **OMM**: Hat relative St√§rken in der Kategorie Dozenten und Praxisbezug.\n\"\"\"\n\nprint(interpretations)"
  },
  {
    "objectID": "code/2_scraping_studycheck.html",
    "href": "code/2_scraping_studycheck.html",
    "title": "Scraping Studycheck",
    "section": "",
    "text": "Create virtual environment (once)\nconda create -n scraping python=3.11 pip\nActivate environment\nconda activate scraping\nInstall modules (once)\npip install jupyter ipykernel pandas selenium webdriver-manager beautifulsoup4 nltk spacy \nImport modules (always)\n\nimport time\nimport pandas as pd\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom bs4 import BeautifulSoup"
  },
  {
    "objectID": "code/2_scraping_studycheck.html#setup",
    "href": "code/2_scraping_studycheck.html#setup",
    "title": "Scraping Studycheck",
    "section": "",
    "text": "Create virtual environment (once)\nconda create -n scraping python=3.11 pip\nActivate environment\nconda activate scraping\nInstall modules (once)\npip install jupyter ipykernel pandas selenium webdriver-manager beautifulsoup4 nltk spacy \nImport modules (always)\n\nimport time\nimport pandas as pd\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom bs4 import BeautifulSoup"
  },
  {
    "objectID": "code/2_scraping_studycheck.html#scraping",
    "href": "code/2_scraping_studycheck.html#scraping",
    "title": "Scraping Studycheck",
    "section": "Scraping",
    "text": "Scraping\n\ndef get_page_reviews(driver, page_url):\n    \"\"\"Fetches reviews from a single page using Selenium.\"\"\"\n    driver.get(page_url)\n    time.sleep(5)  # Wait for the page to load and for any pop-ups to appear\n\n    # Close the cookie consent pop-up if it appears\n    try:\n        consent_button = driver.find_element(By.XPATH, '//button[text()=\"Alles akzeptieren\"]')\n        consent_button.click()\n    except Exception as e:\n        print(f\"No cookie consent pop-up found: {e}\")\n\n    # Extract the page source and parse with BeautifulSoup\n    page_source = driver.page_source\n    soup = BeautifulSoup(page_source, 'html.parser')\n    review_elements = soup.select('.item-text')\n\n    reviews = [review.get_text(strip=True) for review in review_elements]\n    return reviews\n\n\ndef scrape_studycheck(base_url, total_pages):\n    \"\"\"Scrapes reviews from StudyCheck across multiple pages using Selenium.\"\"\"\n    all_reviews = []\n\n    # Setup Selenium WebDriver\n    service = Service(ChromeDriverManager().install())\n    driver = webdriver.Chrome(service=service)\n\n    for page_num in range(1, total_pages + 1):\n        page_url = f\"{base_url}/seite-{page_num}\"\n        print(f\"Scraping page {page_num}...\")\n        reviews = get_page_reviews(driver, page_url)\n        all_reviews.extend(reviews)\n        time.sleep(2)  # Polite delay to avoid overloading the server\n\n    driver.quit()\n    return all_reviews\n\n\n\n# Define the base URLs and total pages for each study program\nstudy_programs = [\n    {\n        \"name\": \"Medienmanagement Mittweida\",\n        \"base_url\": \"https://www.studycheck.de/studium/medienmanagement/hs-mittweida-429/bewertungen\",\n        \"total_pages\": 60\n    },\n    {\n        \"name\": \"Online Medien Management HdM\",\n        \"base_url\": \"https://www.studycheck.de/studium/medienmanagement/hdm-stuttgart-15774/bewertungen\",\n        \"total_pages\": 30\n    },\n\n    {\n        \"name\": \"Medien und Kommunikationsmanagement Macromedia Hochschule\",\n        \"base_url\": \"https://www.studycheck.de/studium/medienmanagement/hs-macromedia-14035/bewertungen\",\n        \"total_pages\": 97\n    }\n\n    ,\n    {\n        \"name\": \"Digital und Medienwirtschaft HdM\",\n        \"base_url\": \"https://www.studycheck.de/studium/medienwirtschaft/hdm-stuttgart-16089/bewertungen\",\n        \"total_pages\": 60\n    }\n]\n\nThis script may take while to finish\n\n\n# Scrape reviews for each study program and save to separate CSV files\nfor program in study_programs:\n    print(f\"Scraping reviews for {program['name']}...\")\n    reviews = scrape_studycheck(program[\"base_url\"], program[\"total_pages\"])\n    df = pd.DataFrame(reviews, columns=['Review'])\n    csv_filename = f\"{program['name'].replace(' ', '_').lower()}_reviews.csv\"\n    df.to_csv(csv_filename, index=False)\n    print(f\"Scraping completed and saved to {csv_filename}\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome üëã",
    "section": "",
    "text": "Welcome to the lab ‚Äú‚Äú\n\nIn this lab you will ‚Ä¶\n\n\n\n\n\n\nImportant\n\n\n\nMake sure you meet all the requirements and have read the lecture slides before you start with the assignments\n\n\nWhat you will learn in this lab:\n\nHow to set up a MySQL database\nUse SQL for data analysis\nMake use of Pandas for data analysis in Python\nUse Altair for data visualization in Python"
  },
  {
    "objectID": "slide.html",
    "href": "slide.html",
    "title": "Slides",
    "section": "",
    "text": "The following tutorials are mainly based on ‚Äú‚Äù provided by\nTxt"
  },
  {
    "objectID": "slide.html#webscraping-basics",
    "href": "slide.html#webscraping-basics",
    "title": "Slides",
    "section": "1 Webscraping Basics",
    "text": "1 Webscraping Basics\n‚èØ Webscraping basics codelab"
  },
  {
    "objectID": "slide.html#scraping-quotes",
    "href": "slide.html#scraping-quotes",
    "title": "Slides",
    "section": "2 Scraping quotes",
    "text": "2 Scraping quotes\nAccept the invitation to application exercise Nr. 1 in Moodle: üíª scraping-quotes"
  },
  {
    "objectID": "slide.html#twitter-api",
    "href": "slide.html#twitter-api",
    "title": "Slides",
    "section": "3 Twitter API",
    "text": "3 Twitter API\nOpen this Twitter setup tutorial on GitHub.\nüíª Application exercise: Twitter API"
  },
  {
    "objectID": "slide.html#txt",
    "href": "slide.html#txt",
    "title": "Slides",
    "section": "4 Txt",
    "text": "4 Txt\nIn this tutorial, you‚Äôll learn:\n\n\n\n\n\n\n\nüñ•Ô∏è Presentation\nüíª Jupyter Notebook"
  }
]